---
title: "Heart Disease Prediction"
author: "Thomas Rowley"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
      number_sections: false
      extra_dependencies: "bbm"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, out.width="80%",
                      fig.align="center", fig.pos = 'H', extra.out="")
library(ggthemes)
library(ggplot2)
library(ggExtra)
library(gridExtra)
library(GGally)
library(kableExtra)
library(klaR)
require(ggord)
library(cvms)
library(caret)
library(covequal)
library(pander)
library(wesanderson)
library(moments)
library(corrplot)
library(nortest)

theme_set(theme_minimal())
theme_set(theme_get() + theme(text = element_text(family = 'serif')))
set.seed(1234567890)
```

<!-- 

#########################################################
EXPLORATORY DATA ANALYSIS SECTION POLISHED FOR THE REPORT
DO NOT INCLUDE IF ANYONE HAS WRITTEN FOR THIS SECTION
#########################################################

# Exploratory Data Analysis

Histograms, notched box plots, and Q-Q plots will be generated from the heart disease data, considering particularly the numerical variables Age, Blood pressure, Cholesterol, and Maximum heart rate. This will be done to visualise the distribution of the data, and check for if the data is Gaussian normally distributed. These plots will be found in Figures \@ref(fig:histPlots), \@ref(fig:boxPlots), and \@ref(fig:qqPlots) respectively.

```{r, warning = FALSE}
# Load libraries
# Read the data
HDdata <- read.csv("Heart_Disease_Prediction.csv")

# Subset your data
subset_HDdata <- HDdata[, c("Age", "BP", "Cholesterol", "Max.HR")]

# Define colors for each category
pal <- wes_palette("Moonrise3", 5)
colors <- c("Age" = pal[1], "BP" = pal[2], "Cholesterol" = pal[3], 
            "Max.HR" = pal[5])

histList <- list()
boxList <- list()
qqList <- list()

# Create and display histograms, boxplots, and Q-Q plots for all columns
for (col in colnames(subset_HDdata)) {
  # Create histogram with density curve, mean, and standard deviation lines
  histogram_plot <- ggplot(subset_HDdata, aes(x = .data[[col]])) +
    geom_histogram(binwidth = 5, fill = colors[col], color = "black") +
    geom_density(aes(y = ..density..), kernel = "epanechnikov", size = 1, col = "purple", alpha = 0.2, adjust = 2) +
    geom_vline(xintercept = mean(subset_HDdata[[col]]), col = "red", size = 1, alpha = 0.5) +
    labs(title = paste(col), x = col, y = "Frequency") + 
    theme_minimal()
  
  # Create and display boxplot
  boxplot_plot <- ggplot(subset_HDdata, aes(y = .data[[col]]), notch=TRUE) +
    geom_boxplot(fill = colors[col]) +
    labs(title = paste(col), x = "", y = col) +
    theme_minimal()
  
    # Create and display Q-Q plot
  qq_plot <- ggplot(subset_HDdata, aes(sample = .data[[col]])) +
    stat_qq(color=colors[col]) + stat_qq_line() + # added Q-Q line as it was missing from the original plots
    labs(title = paste(col), x = "Theoretical Quantiles", y = "Sample Quantiles") +
    theme_minimal()
  
  # Potential option 2 (grouped by variable)
  histList[[col]] <- histogram_plot
  boxList[[col]] <- boxplot_plot
  qqList[[col]] <- qq_plot

}
```

```{r histPlots, fig.cap="Histograms for the numerical variables of the heart disease dataset."}
gridExtra::grid.arrange(grobs=histList)
```

From Figure \@ref(fig:histPlots), the histograms for age appears to be the most normal with the least significant outliers, whereas cholesterol, maximum heart rate, and blood pressure have distributions that are visually not normally distributed, with significant outliers.

The blood pressure histogram has many peaks at the 10 mmHg marks. This is a result of "zero end-digit preference", a phenomenon where blood pressure readings are rounded to the nearest ten. This is known to occur especially around treatment cutoffs (120 mmHg is considered 'elevated', 130mmHg is 'stage 1 hypertension', and 140mmHg is 'stage 2 hypertension')^[https://www.cdc.gov/bloodpressure/facts.htm]. 150 of the 270 observations, approximately 56%, are given as a multiple of 10. Most blood pressure indicators give guidelines to measure to the nearest 2 mmHg. The potential rounding of data indicates a lack of normality in the data since many observations may have been rounded up or down significantly.

Next, box plots should be generated to analyse the distribution of data for each variable further.

```{r boxPlots, fig.cap="Notched box plots for the numerical variables of the heart disease dataset."}
# Grid arrangement of box plots
gridExtra::grid.arrange(grobs=boxList, ncol=2)
```

The box plots from Figure \@ref(fig:boxPlots) shows that blood pressure and cholesterol values have noted outliers to the higher end of values, where maximum heart rate has one outlier on the lower end. The outlier for maximum heart rate could be due to a patient already receiving treatment for heart disease, and being on beta blockers, a commonly prescribed drug to lower heart rate and reduce the risk of heart attack.^[https://www.heartfoundation.org.nz/your-heart/heart-treatments/medications/beta-blockers]

Next, normal Q-Q plots should be generated for each variable to check for the normality of residuals.

```{r qqPlots, fig.cap="Normal Q-Q plots for the numerical variables of the heart disease dataset."}
# Grid arrangement of Q-Q plots
gridExtra::grid.arrange(grobs=qqList)
```

The normal Q-Q plots from Figure \@ref(fig:qqPlots) for the variables suggest normality in residuals for all variables may be present. The only deviations from the Q-Q line are at the high ends data where outliers where noted to be present in cholesterol, maximum heart rate, and blood pressure.

Summary statistics, the covariance matrix, and the correlation matrix are provided below in Tables \@ref(tab:summStat), \@ref(tab:covMat), and \@ref(tab:corMat) respectively.

```{r}
# Print Summary Statistics
# Restructured into a nice looking table, with skewness and kurtosis added
# Only looks good once knitted
summ <- as.data.frame.matrix(sub('.*:','', summary(subset_HDdata)))
sample_size <- rep(length(subset_HDdata[,1]), ncol(subset_HDdata))
summ <- rbind(sample_size, summ, round(skewness(subset_HDdata),4), round(kurtosis(subset_HDdata),4))
rownames(summ) <- c("Sample size", "Minimum", "1st quartile", "Median", "Mean", "3rd quartile",
                    "Maximum", "Skewness", "Kurtosis")
knitr::kable(summ, booktabs=TRUE, label="summStat",
             caption="Exploratory data analysis summary for the numerical variables of heart disease data.") %>% kable_styling(latex_options = "HOLD_position") # Table 1
```

```{r}
knitr::kable(cov(subset_HDdata), booktabs=TRUE, label="covMat",
             caption="Covariance matrix of the numerical variables of the heart disease dataset.") %>% 
  kable_styling(latex_options = "HOLD_position")
```

```{r}
knitr::kable(cor(subset_HDdata), booktabs=TRUE, label="corMat",
             caption="Covariance matrix of the numerical variables of the heart disease dataset.") %>%
  kable_styling(latex_options = "HOLD_position")
```

A correlogram is generated as a visualisation of the correlation matrix between variables and presented in Figure \@ref(fig:correlogramHD).

```{r correlogramHD, fig.cap="Correlogram of numerical variables in the heart disease dataset."}
corrplot(cor(subset_HDdata), type = "upper", 
         method = "square", 
         addCoef.col = "white", 
         tl.col = "black", tl.srt = 45)
```

The only notable correlation is with age and maximum heart rate, with a moderate negative correlation of $-0.4$. This is expected as it is known that as a person ages, the condition of the heart and therefore its maximum capacity for pumping blood, decreases.^[https://www.msdmanuals.com/en-nz/home/heart-and-blood-vessel-disorders/biology-of-the-heart-and-blood-vessels/effects-of-aging-on-the-heart-and-blood-vessels]

Next the data should be tested for normality using the Anderson-Darling test. The results will be found in Table \@ref(tab:adPval).

```{r}
pvalAD <- list()
for (col in colnames(subset_HDdata)) {
  pvalAD[[col]] <- nortest::ad.test(subset_HDdata[[col]])$p.value
}

pvalAD <- as.data.frame(pvalAD)
knitr::kable(pvalAD, booktabs=TRUE, label="adPval",
       caption="Anderson-Darling test p-values for normality.") %>%
  kable_styling(latex_options = "HOLD_position")
```

There is evidence to reject normality for all numerical variables by the Anderson-Darling test, at the $\alpha = 0.05$ significance level. The data can be log-transformed to see if the distribution of the log-transformed data is normal, and more Anderson-Darling tests can be performed. The results for the log-transformed data's Anderson-Darling tests can be found in \@ref(tab:logadPval).

```{r logadPval}
pvalADlog <- list()
for (col in colnames(subset_HDdata[,-c(5)])) {
  pvalADlog[[col]] <- nortest::ad.test(log(subset_HDdata[[col]]))$p.value
}

pvalADlog <- as.data.frame(pvalADlog)
knitr::kable(pvalADlog, booktabs=TRUE,
       caption="Anderson-Darling test p-values for normality on log-transformed data.") %>%
  kable_styling(latex_options = "HOLD_position")

```

Transforming cholesterol and blood pressure give better p-values for testing whether or not the distribution of the log-transformed data is normal, so log-transformations to cholesterol and blood pressure should be applied.
-->

# Heart disease prediction

The first question we seek to answer is if we can predict heart disease using clinical variables. The variables that will be used are: Age, sex, maximum heart rate, cholesterol level, blood pressure, and type of chest pain.

A pairs plot will be generated to visualise potential differences in the distributions of the aforementioned variables that are numeric, i.e. age, blood pressure, cholesterol, and maximum heart rate, depending on heart disease presence or absence. The plot can be found in Figure \@ref(fig:pairs).

```{r}
hd <- read.csv("Heart_Disease_Prediction.csv")
hd <- as.data.frame(hd[,-1])
hdTransformed <- hd
hdTransformed$BP <- log(hdTransformed$BP)
hdTransformed$Cholesterol <- log(hdTransformed$Cholesterol)
colnames(hdTransformed) <- c("Age", "Sex", "Chest.pain.type", "log(BP)", "log(Cholesterol)", "FBS.over.120", "EKG.results", "Max.HR", "Exercise.angina", "ST.depression", "Slope.of.ST", "Number.of.vessels.fluro", "Thallium", "Heart.Disease")
```

```{r pairs, fig.cap="Pairs plot for the continuous numeric variables.", message=FALSE}
ggpairs(hdTransformed, aes(colour=Heart.Disease, alpha=.5), columns=c("Age", "log(BP)", "log(Cholesterol)", "Max.HR")) +
  theme_pander(base_size = 8)
```

From the pairs plot it is seen that the distribution of age and maximum heart rate between patients with and without heart disease differ significantly. The age plot shows that heart disease is more prevalent in older people. The maximum heart rate plot shows that the maximum heart rate for those with heart disease is more normally distributed, and tends to be lower, than those without heart disease. The plots for the log-transformed cholesterol and log-transformed blood pressure are similar in shape, with slight shifts to higher values for those with heart disease. From these plots there is some evidence that classes of heart disease may be separable due to differences in distribution between predictor values.

## Linear discriminant analysis

A linear discriminant analysis will be performed on the heart disease dataset in order to predict the heart disease variable using age, sex, chest pain type, the log-transformed blood pressure and cholesterol variables, and maximum heart rate. Test and training datasets will be obtained by randomly sampling the data with replacement, with $80\%$ and $20\%$ probabilities respectively.

```{r LDA}
ind <- sample(c("Tr", "Te"),
              nrow(hd),
              replace=TRUE,
              prob=c(0.8,0.2))

Train <- hd[ind=="Tr",]
Test <- hd[ind=="Te",]

LDA <- lda(Heart.Disease ~ Age + log(BP) + log(Cholesterol) + Max.HR +
             as.factor(Chest.pain.type) + as.factor(Sex), data=Train)
```

### Results

#### Model fitting:

The results of the linear discriminant analysis can be seen in Figure \@ref(fig:LDAhist) and Figure \@ref(fig:assignHist). In the figures, overlap in the plots indicates that those data points could be assigned either with heart disease, or assigned to not have heart disease, meaning misclassification errors are possible. Both figures represent the same data, where one is a histogram of each class on its own axis, and the other is a density plot where both classes' data is on the same axis.

```{r LDAhist, fig.cap="Histogram of assigned class of heart disease."}
Pred <- predict(LDA)
ldahist(data=Pred$x, g=Train$Heart.Disease)
```

```{r assignHist, fig.cap="Density plot of assigned class of heart disease."}
df <- data.frame(Pred$x, Pred$class)
ggplot(df, aes(x=Pred$x, fill=Pred$class))+geom_density(alpha=0.5) +
  labs(fill="Class") + ylab("Density") + xlab("x")
```

From the figures it is clearly seen that there is overlap in predicted values for the data, meaning misclassification of patients' heart disease status is present and possible in the model.

An assumption for performing linear discriminant analysis is that covariance matrices of variables, between the data where heart disease is present, and the data where heart disease is absent, are similar. The test for covariance equality is found in Table \@ref(tab:covEquality).

```{r covEquality}
HDabsence <- as.matrix(subset(hd, hd$Heart.Disease=="Absence")[,-c(14)], ncol=13)
HDpresence <- as.matrix(subset(hd, hd$Heart.Disease=="Presence")[,-c(14)], ncol=13)
pander(as.data.frame(test_covequal(HDabsence, HDpresence)),
       caption="(\\#tab:covEquality) Test results for comparing covariance matrices for 
       absence and presence of heart disease for equality.")
```

The covariance equality test is performed at the $\alpha = 0.05$ significance level, with null hypothesis that covariance matrices are equal. The associated p-value for this test is $p=0.07649$. Since $p > \alpha$, there is insufficient evidence to reject that covariance matrices are equal. The assumption required for linear discriminant analysis thus holds.

#### Prediction:

Statistics for the misclassifications can be obtained via confusion matrices. Using the test data, we can report a realistic confusion matrix for the model, which is presented in Table \@ref(tab:realisticConfMat).

```{r realisticConfMat}
RealisticPredicted <- predict(LDA, Test)$class
RCM <- table(RealisticPredicted, Actual=Test$Heart.Disease)
LDACMstats <- confusionMatrix(RCM, positive = "Presence")
knitr::kable(LDACMstats$table, booktabs=TRUE, 
             caption="Confusion matrix for predicting heart disease using linear discriminant analysis.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The linear discriminant model has wrongly assigned 7 of 43 patients without heart disease as having heart disease, and 3 of 27 patients with heart disease as not having heart disease, errors of $22.58\%$ and $7.69\%$ respectively.

More detailed statistics concerning the confusion matrix are presented in Table \@ref(tab:LDAConfMatStats) and Table \@ref(tab:LDAOverallStats).

```{r LDAOverallStats}
knitr::kable(scales::percent(LDACMstats$overall, accuracy=0.01), booktabs=TRUE, 
             caption="Overall statistics for predicting heart disease using linear discriminant analysis.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The linear discriminant model has an overall accuracy of $85.71\% \pm 10.42\%$. The accuracy of the model is tested against the No Information Rate, which is the largest proportion of any class in the model, i.e. $\text{P}(Absence)$, at the $\alpha=5\%$ significance level. The p-value for the significance of this model with respect to the No Information Rate of $55.71\%$ is $p\approx0.00\%$ at the $\alpha = 5\%$ significance level, meaning the linear discriminant analysis is highly significant in its prediction power for heart disease over the no-information model.

```{r LDAConfMatStats}
knitr::kable(scales::percent(LDACMstats$byClass, accuracy=0.01), booktabs=TRUE, 
             caption="Prediction statistics for predicting heart disease using linear discriminant analysis.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The model has sensitivity of $77.42\%$, specificity of $92.31\%$, and a balanced accuracy of $84.86\%$. This means it can predict someone with heart disease as having it with $77.42\%$ certainty, and someone without heart disease as not having it with $92.31\%$ certainty. Equivalently, this means the model has a false positive rate of $22.58\%$ and a false negative rate of $7.69\%$. In terms of the research question, this means the linear discriminant analysis fails to detect heart disease in patients that do have heart disease $7.69\%$ of the time.

A partition matrix is generated considering all numerical variables as before. Red points represent that the observation has been misclassified, and black points represent correctly classified observations. The partition plot can be found in Figure \@ref(fig:HDPredictpartimat).

```{r HDPredictpartimat, fig.cap="Partition plot of numerical variables used in the linear discriminant analysis."}
partimat(as.factor(Heart.Disease) ~ Age + log(BP) + log(Cholesterol) + Max.HR,
         data=Train, method="lda")
```

The approximate error rate for classification is highest at $43\%$ between blood pressure and age, and lowest at $29.5\%$ between maximum heart rate and cholesterol. This gives a range of classification error between $29.5\%-43\%$ between the numerical variables of the linear discriminant model.

## Bayesian classification

The prior probabilities of having heart disease and not having heart disease are different in the data, with approximately $45\%$ of patients having heart disease. Linear discriminant analysis where classes are unbalanced could mean that large amounts of data is misclassified by the linear discriminant. To account for this, a Bayesian classification can be used instead, as this accounts for the prior probability differences. It will be assumed that the data belongs to the Gaussian normal distribution.

```{r}
#necessary transformation for factors
Train$Chest.pain.type <- as.factor(Train$Chest.pain.type)
Train$Sex <- as.factor(Train$Sex)
Test$Chest.pain.type <- as.factor(Test$Chest.pain.type)
Test$Sex <- as.factor(Test$Sex)

# log transformation of test and train data
Train$BP <- log(Train$BP)
Train$Cholesterol <- log(Train$Cholesterol)
Test$BP <- log(Test$BP)
Test$Cholesterol <- log(Test$Cholesterol)

Bayes <- naivebayes::gaussian_naive_bayes(x=data.matrix(Train[,c(1,2,3,4,5,8)]), y=Train$Heart.Disease, data=Train)
```

From the data the prior probabilities of an observation belonging to either distribution are given as $\text{P}(\text{Absence}) = \frac{111}{200} = 0.555$ and $\text{P}(\text{Presence}) = \frac{89}{200} = 0.445$.

### Results

#### Model fitting:

Bayesian classification can be applied to the test data for the variables age, sex, chest pain type, blood pressure, cholesterol level, and maximum heart rate, the same as in the linear discriminant analysis. Its performance will be compared to that of the linear discriminant analysis. Tables of results can be found in Table \@ref(tab:bayesConfMat), Table \@ref(tab:bayesOverallStats), and Table \@ref(tab:bayesConfMatStats).

#### Prediction:

```{r bayesConfMat}
BayesPred <- predict(Bayes, newdata=data.matrix(Test[,c(1,2,3,4,5,8)]), type="class")
RCMBayes <- table(BayesPred, Actual=Test$Heart.Disease)
BayesCMstats <- confusionMatrix(RCMBayes, positive="Presence")
knitr::kable(BayesCMstats$table, booktabs=TRUE, 
             caption="Confusion matrix for predicting heart disease using the Bayesian classification method.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The Bayesian classification method wrongly assigned 7 of 39 patients without heart disease as having heart disease, and misclassified 24 of 31 patients with heart disease as not having it, errors of $17.95\%$ and $29.03\%$ respectively. These error values are  different than those of the linear discriminant analysis, which had errors of $22.58\%$ and $7.69\%$. Notably, the Bayesian classification has less error when identifying absence of heart disease than the linear discriminant analysis, but much more error when identifying heart disease in patients, approximately 3.7 times worse.

```{r bayesOverallStats}
knitr::kable(scales::percent(BayesCMstats$overall, accuracy=0.01), booktabs=TRUE, 
             caption="Overall statistics for predicting heart disease using the Bayesian classification method.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The Bayesian classification has an overall accuracy of $77.14\% \pm 11.59\%$. The accuracy of this model is tested against the null hypothesis that the model is no better than the largest proportion of classes, the No Information Rate. This hypothesis is tested at the $\alpha=5\%$ level, and the corresponding p-value is $0.02\%$. Since $p < \alpha$, there is sufficient evidence to suggest the model is better at predicting levels of heart disease than the no information model. This means the Bayesian classification method is significant in its prediction power for heart disease by this metric. 

```{r bayesConfMatStats}
knitr::kable(scales::percent(BayesCMstats$byClass, accuracy=0.01), booktabs=TRUE, 
             caption="Prediction statistics for predicting heart disease using the Bayesian classification method") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The Bayesian classification has sensitivity of $70.97\%$, specificity of $82.05\%$, and a balanced accuracy of $76.51\%$ for predicting the presence of heart disease. Equivalently, this means the model has a false positive rate of $29.03\%$ and a false negative rate of $17.95\%$. In terms of the research question, this means the Bayesian classification assuming the data came from the Gaussian normal distribution does not detect heart disease in patients that have it $17.95\%$ of the time.

A partition matrix can also be generated for this data on its numerical variables.

```{r BayesPartimat, fig.cap="Partition plot of the numerical variables used in the Bayesian classification."}
partimat(as.factor(Heart.Disease) ~ Age + log(BP) + log(Cholesterol) + Max.HR,
         data=Train, method="naiveBayes")
```

The approximate error rate for classification is highest at $43.5\%$ between the log of blood pressure and log of cholesterol, and lowest at $28.0\%$ between maximum heart rate and the log cholesterol. This gives a range of classification error between $28.0\%-43.5\%$ between the numerical variables of the Bayesian classification, slightly better than that of the linear discriminant analysis.

## Comparison of approaches

Important summary statistics that have been previously calculated are collated in Table \@ref(tab:compsummary).

```{r compsummary}
sens <- c("77.42%", "70.97%")
fp <- c("22.58%", "29.03%")
spec <- c("92.31%", "82.05%")
fn <- c("7.69%", "17.95")
oAcc <- c("85.71%", "77.14%")
bAcc <- c("84.86%", "76.51%")
compSummary <- data.frame(sens, fp, spec, fn, oAcc, bAcc)
compSummary <- t(compSummary)
colnames(compSummary) <- c("LDA", "Bayesian")
rownames(compSummary) <- c("Sensitivity", "False Positive Rate", "Specificity", "False Negative Rate", "Overall Accuracy", "Balanced Accuracy")
knitr::kable(compSummary, booktabs=TRUE, 
             caption="Summary of important statistics for model comparison for heart disease prediction.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The linear discriminant analysis has lower false positive rates, lower false negative rates, higher overall accuracy, and higher balanced accuracy compared to the Bayesian classification assuming Gaussian normal distribution approach. By all metrics obtained from confusion matrices, the linear discriminant analysis provides a more accurate and more useful model for predicting the presence of heart disease in patients considering their age, sex, maximum heart rate, the log-transformation of their cholesterol and blood pressure readings, and their type of chest pain. It correctly classifies patients with and without heart disease more often than the Bayesian classification approach does, as it makes less errors.

The difference in performance of each model could be attributed to a number of possibilities. The Bayesian classification we used assumed data belonged to the Gaussian normal distribution, which according to the Anderson-Darling tests was only likely for the log-transformed cholesterol value. It also assumed independence of predictor variables, which may have been violated by covariances between predictors in the data. The linear discriminant analysis however does not assume independence between predictor values, instead assuming that covariances matrices are equal. Linear discriminant analysis like Bayesian classification, assumes normality of data, so the disparity between results of the models is likely to be due to the lack of independence between predictor variables.
This is evidenced by correlation between predictors being prevalent as seen in Figure \@ref(fig:pairs).